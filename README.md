# MetaBoost-RF: стековый бустинг поверх Random Forest для повышения accuracy  
### Кейс: классификация Fashion-MNIST (784 признака) классическими ансамблями

## Аннотация
Мы исследуем подход, в котором базовый алгоритм RandomForestClassifier усиливается стековым бустингом за счёт гетерогенной совокупности деревья-градиентных алгоритмов (XGBoost, LightGBM) и мета-модели. Идея проста: разные ансамблевые модели по-разному аппроксимируют сложные границы решений на плоском (не CNN) представлении изображений; стек объединяет их вероятностные выходы, повышая устойчивость и итоговую точность.

## Вклад
1) Конструкция ансамбля «ансамбль ансамблей»: RF + XGBoost + LightGBM → Stacking (логистическая регрессия).  
2) Честная OOF-валидация внутри стекера (cv>1).  
3) Репродуцируемый пайплайн: загрузка, обучение, метрики, артефакты, графики.

## Датасет
**Fashion-MNIST** (10 классов, 28×28, grayscale). Используем плоское представление 784 признака, что делает задачу показательной для деревьев-бустингов без сверточных сетей.

## Метод
Пусть \( f_{RF}, f_{XGB}, f_{LGBM} \) — базовые обучаемые алгоритмы, выдающие \(p(y|x)\).  
Мета-модель \( g \) обучается на OOF-предсказаниях \([p_{RF}, p_{XGB}, p_{LGBM}]\) и минимизирует кросс-энтропийный риск.  
Финальный скоринг: \( \hat{y} = \arg\max g([p_{RF}, p_{XGB}, p_{LGBM}]) \).

## Как запустить
```bash
pip install -r requirements.txt
python metaboost_rf.py --test-size 0.2 --cv 5 --rf-estimators 300 --xgb-estimators 400 --lgbm-estimators 400
python plot_report.py
